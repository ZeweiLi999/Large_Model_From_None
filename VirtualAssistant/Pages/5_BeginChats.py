import streamlit as st
<<<<<<< HEAD
from LLM.Chat_reponse import Chat_reponse
import time

def stream_data(response):
    for word in response:
        yield word
        time.sleep(0.02)

options = ["è‹±è¯­æ•™å¸ˆæ¨¡å‹","ç”„å¬›æ¨¡å‹"]

select_model = st.selectbox("è¯·é€‰æ‹©ä½ çš„æ¨¡å‹" + ':star2:', options, index=0)

st.divider()  # Draws a horizontal rule

with st.chat_message("ai"):
    st.write(f"æˆ‘æ˜¯{select_model}\n\nä½ æƒ³é—®æˆ‘äº›ä»€ä¹ˆå‘€ï¼Ÿ")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# React to user input
if input := st.chat_input("Say Something"):
    # Display user message in chat message container
    st.chat_message("user").markdown(input)
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": input})

    response = Chat_reponse(input)
    # Display assistant response in chat message container
    with st.chat_message("assistant"):
        st.write_stream(stream_data(response))
    # Add assistant response to chat history
    st.session_state.messages.append({"role": "assistant", "content": response})
=======


prompt = st.chat_input("Say something")
with st.chat_message("user"):
    st.write(f"é¡¾æ—­ä¸œæ˜¯è°å‘€")
with st.chat_message("ai"):
    st.write("çš‡ä¸Šèº«è¾¹çš„ä¸€ä¸ªå°å¤ªç›‘ğŸ‘‹")

with st.chat_message("user"):
    st.write(f"ææ³½å¨æ˜¯è°å‘€")
with st.chat_message("ai"):
    st.write("ä»–æ˜¯çš‡ä¸Šæ–°è¿‘å¸¦å›æ¥çš„ä¸€ä¸ªå°ä¸»å­ï¼Œä»–çš„æ¯äº²æ˜¯å½“å¹´è¢«çš‡ä¸Šå® å¹¸çš„å®«å¥³ï¼Œçš‡ä¸Šå¾ˆå® ä»–ã€‚")
>>>>>>> 0681856a209f6f3b029501abd7d9ea22b5097a5a
