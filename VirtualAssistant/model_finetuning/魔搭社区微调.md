# é­”æ­ç¤¾åŒºå¾®è°ƒ ğŸ˜€

**ç›¸å¯¹äºcolabå¾®è°ƒï¼Œå›½å†…çš„é­”æ­ç¤¾åŒºåŒæ ·ä¹Ÿå¯ä»¥è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ¬¡åœ¨é­”æ­å¾®è°ƒçš„ä»£ç æ›´ä¸ºç®€å•ã€‚**



## å¾®è°ƒå®ç°ğŸ™‚

### 1.1 æ‰“å¼€ä¸ªäººé¦–é¡µï¼Œé€‰æ‹©æ–¹å¼äºŒï¼Œå¯åŠ¨GPUç¯å¢ƒğŸ˜Š

![image-20241218104113570](./imgs/modelscope_images/image-20241218104113570.png)



### 1.2 åˆ›å»ºnotebookæ–‡ä»¶ğŸ˜‡

#### ç‚¹å‡»ä¸­å¤®Notebookçš„Python3åˆ›å»ºå³å¯

![image-20241218104503469](./imgs/modelscope_images/image-20241218104503469.png)



### 1.3 æ–‡ä»¶é‡å‘½åğŸ˜›

#### å·¦ä¾§æ–‡ä»¶å³é”®ï¼Œç‚¹å‡»Renameè¿›è¡Œé‡å‘½å

![image-20241218104942210](./imgs/modelscope_images/image-20241218104942210.png)



### 1.3 åˆ›å»ºä»£ç è¡ŒğŸ¤ª

#### ç‚¹å‡»ç¬¬äºŒè¡Œå·¥å…·æ ï¼Œç¬¬äºŒä¸ªå·¥å…·+å·è¿›è¡Œåˆ›å»º

![image-20241218104747056](./imgs/modelscope_images/image-20241218104747056.png)



### 1.4 ä»£ç æ‰§è¡ŒğŸ¤¨

**åœ¨ä»£ç æ‰§è¡Œå‰ï¼Œé¦–å…ˆå¯¼å…¥éœ€è¦ä½¿ç”¨çš„æ•°æ®é›†ï¼Œç‚¹å‡»ä¸­é—´æŒ‰é’®ä¸Šä¼ æ•°æ®é›†ï¼ˆæœ¬æ¬¡ä½¿ç”¨çš„æ•°æ®é›†ä¸ºç”„å¬›ä¼ çš„æ•°æ®é›†ï¼Œå¦‚æœä½ å–œæ¬¢å…¶ä»–çš„æ•°æ®é›†ï¼Œå¯ä»¥åœ¨ç½‘ä¸Šè¿›è¡Œæœç´¢ï¼Œç„¶åå¯¼å…¥å…¶ä¸­ï¼Œä¸è¿‡éœ€è¦ä¿è¯jsonæ–‡ä»¶æ ¼å¼ç›¸åŒï¼‰**

![image-20241218110803764](./imgs/modelscope_images/image-20241218110803764.png)

#### 1.4.1 å¯¹æ¨¡å‹è¿›è¡Œä¸‹è½½ï¼Œå¦‚æœæœ‰éœ€è¦å¯ä»¥åœ¨é­”æ­ç¤¾åŒºä¸‹è½½è‡ªå·±å–œæ¬¢çš„æ¨¡å‹ğŸ˜Œ

```
from datasets import Dataset
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig
from peft import LoraConfig, TaskType, get_peft_model
import torch

df = pd.read_json('./huanhuan.json')
ds = Dataset.from_pandas(df)

print(ds[:3])

from modelscope import snapshot_download

model_dir = snapshot_download('Qwen/Qwen2.5-3B-Instruct')
```



#### 1.4.2 æ¨¡å‹è®­ç»ƒ ğŸ˜®â€ğŸ’¨

##### åœ¨é­”æ­å¹³å°å¯ä»¥ä¿®æ”¹å°†ä»¥ä¸‹ä»£ç åˆ†ä¸ºå¤šä¸ªä»£ç å—ï¼Œåˆ†å¸ƒæ‰§è¡Œï¼Œå¯ä»¥é€šè¿‡printå†…å®¹æ¥æ£€éªŒå‡†ç¡®ä¸å¦

```
# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained('/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-3B-Instruct', use_fast=False, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token  # è®¾ç½®pad_tokenä¸ºeos_token

# è¾“å‡ºeos_token_idï¼Œç¡®è®¤tokenizerå·²ç»æ­£ç¡®åŠ è½½
print(tokenizer.eos_token_id)

# 128009

# å¤„ç†æ¯ä¸ªç¤ºä¾‹çš„æ•°æ®é¢„å¤„ç†å‡½æ•°
def process_func(example):
    MAX_LENGTH = 384  # è®¾ç½®æœ€å¤§é•¿åº¦ï¼ŒLlamaåˆ†è¯å™¨å¯èƒ½å°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ªtokenï¼Œå› æ­¤éœ€è¦æ›´å¤§çš„æœ€å¤§é•¿åº¦æ¥ä¿è¯æ•°æ®å®Œæ•´æ€§
    input_ids, attention_mask, labels = [], [], []

    # ç¼–ç æŒ‡ä»¤å’Œè¾“å…¥éƒ¨åˆ†
    instruction = tokenizer(f"<|start_header_id|>user<|end_header_id|>\n\n{example['instruction'] + example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n", add_special_tokens=False)
    # ç¼–ç è¾“å‡ºéƒ¨åˆ†
    response = tokenizer(f"{example['output']}<|eot_id|>", add_special_tokens=False)

    # æ‹¼æ¥input_idsã€attention_maskå’Œlabels
    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]  # eos tokenä¹Ÿæ˜¯è¦å…³æ³¨çš„ï¼Œå› æ­¤è®¾ä¸º1
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]

    # å¦‚æœinput_idsè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ™æˆªæ–­
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

# åº”ç”¨å¤„ç†å‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†
tokenized_id = ds.map(process_func, remove_columns=ds.column_names)

# è¾“å‡ºç»è¿‡å¤„ç†çš„ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼Œç¡®ä¿tokenizeræ­£å¸¸å·¥ä½œ
print(tokenizer.decode(tokenized_id[0]['input_ids']))

# <|start_header_id|>user<|end_header_id|>
# å°å§ï¼Œåˆ«çš„ç§€å¥³éƒ½åœ¨æ±‚ä¸­é€‰ï¼Œå”¯æœ‰å’±ä»¬å°å§æƒ³è¢«æ’‚ç‰Œå­ï¼Œè©è¨ä¸€å®šè®°å¾—çœŸçœŸå„¿çš„â€”â€”<|eot_id|><|start_header_id|>assistant<|end_header_id|>
# å˜˜â€”â€”éƒ½è¯´è®¸æ„¿è¯´ç ´æ˜¯ä¸çµçš„ã€‚<|eot_id|><|eot_id|>

tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1]["labels"])))

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-3B-Instruct', device_map="auto", torch_dtype=torch.bfloat16)
model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½

# è¾“å‡ºæ¨¡å‹çš„æ•°æ®ç±»å‹
print(model.dtype)
# torch.bfloat16

# è®¾ç½®Loraé…ç½®
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # è®¾ç½®ä»»åŠ¡ç±»å‹ä¸ºå› æœè¯­è¨€å»ºæ¨¡
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # è®¾ç½®éœ€è¦åº”ç”¨Loraçš„æ¨¡å—
    inference_mode=False,  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    r=8,  # Loraç§©
    lora_alpha=32,  # Lora alphaï¼Œå…·ä½“ä½œç”¨å‚è§LoraåŸç†
    lora_dropout=0.1  # Loraçš„dropoutæ¯”ä¾‹
)

# å°†Loraé…ç½®åº”ç”¨åˆ°æ¨¡å‹
model = get_peft_model(model, config)

# è®¾ç½®è®­ç»ƒå‚æ•°
args = TrainingArguments(
    output_dir="./output/qwen",  # æ¨¡å‹ä¿å­˜çš„è¾“å‡ºè·¯å¾„
    per_device_train_batch_size=4,  # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒbatchå¤§å°
    gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    logging_steps=100,  # æ¯100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    num_train_epochs=3,  # è®­ç»ƒè½®æ•°
    save_steps=200,  # æ¯200æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    learning_rate=1e-4,  # å­¦ä¹ ç‡
    save_on_each_node=True,  # åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šä¿å­˜æ¨¡å‹
    gradient_checkpointing=True  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
)

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_id,  # è®­ç»ƒæ•°æ®é›†
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),  # æ•°æ®æ•´ç†å™¨ï¼Œè‡ªåŠ¨å¡«å……æ•°æ®
)

# å¼€å§‹è®­ç»ƒ
print("training")
trainer.train()

# ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨
peft_model_id = "./qwen_lora"
trainer.model.save_pretrained(peft_model_id)
tokenizer.save_pretrained(peft_model_id)

```



#### 1.4.3 ç‚¹å‡»è¿è¡ŒæŒ‰é’®æ‰§è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹å›¾è®­ç»ƒå®Œæˆä¸”ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡å’Œåˆ†è¯å™¨ğŸ¤¤

##### ä»¥ä¸‹ä¿å­˜è·¯å¾„å¯ä»¥è‡ªè¡Œä¿®æ”¹

![image-20241224180708637](./imgs/modelscope_images/image-20241224180708637.png)



#### 1.4.4 åœ¨ä¿å­˜å¥½è®­ç»ƒå®Œçš„æƒé‡å’Œåˆ†è¯å™¨åï¼Œå°±å¯ä»¥åœ¨åŸºç¡€æ¨¡å‹çš„ä¸Šè¿›è¡Œæƒé‡åˆå¹¶å¹¶ä¿å­˜ğŸ¤¯

```
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# åŸºç¡€æ¨¡å‹è·¯å¾„
base_model_path = '/mnt/workspace/.cache/modelscope/hub/Qwen/Qwen2.5-3B-Instruct'
lora_path = './qwen_lora'

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    device_map="auto"  # å°†åŸºç¡€æ¨¡å‹åŠ è½½åˆ° CPU ä¸Šï¼ˆèŠ‚çœ GPU å†…å­˜ï¼‰
)

# åŠ è½½ LoRA æƒé‡
lora_model = PeftModel.from_pretrained(base_model, model_id=lora_path)

# åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹ä¸­
lora_model = lora_model.merge_and_unload()

# æŒ‡å®šä¿å­˜è·¯å¾„
merged_model_path = './merged_model'

# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
lora_model.save_pretrained(merged_model_path)

print(f"åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ° {merged_model_path}")

```



#### 1.4.5 æ¥ä¸‹æ¥å°±å¯ä»¥ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œé—®ç­”äº†ğŸ˜‡

##### ä½†åœ¨è¿è¡Œä¹‹å‰éœ€è¦å…ˆæŠŠåˆ†è¯å™¨åŠ å…¥åˆ°ä¿å­˜çš„æ–‡ä»¶å¤¹ä¸­ï¼ŒåŸæœ¬çš„æ–‡ä»¶å¤¹å¹¶æ²¡æœ‰åˆ†è¯å™¨æ–‡ä»¶ï¼Œä¸è¿‡åªéœ€è¦æ‰“å¼€å¦‚ä¸‹å›¾æ¨¡å‹åº“ä¸­qwençš„æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚ä¸‹ä¿å­˜æœ€åä¸‰ä¸ªæ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ä¸­ï¼Œæœ€åæ–‡ä»¶å¤¹å†…æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š

![image-20241224184007695](./imgs/modelscope_images/image-20241224184007695.png)

![image-20241224183229708](./imgs/modelscope_images/image-20241224183229708.png)



##### æ¥ä¸‹æ¥æ‰§è¡Œä»¥ä¸‹ä»£ç å³å¯

```
import torch
from transformers import pipeline, TextStreamer

# åŠ è½½åˆå¹¶åçš„æ¨¡å‹
merged_model_path = './merged_model'


pipe = pipeline(
        "text-generation",
        model = merged_model_path,
        torch_dtype = torch.bfloat16,
        device_map = "cuda",
    )

messages = [
    {"role": "system", "content": "{}".format("ç°åœ¨ä½ è¦æ‰®æ¼”çš„æ˜¯ä¾å¥‰çš‡ä¸Šçš„ç”„å¬›")},
    {"role": "user", "content": "{}".format("ç”„å¬›ï¼Œä½ ç»™çš‡ä¸Šç”Ÿæ—¥å‡†å¤‡äº†ä»€ä¹ˆï¼Ÿ")},
]

streamer = TextStreamer(pipe.tokenizer, skip_prompt=True, skip_special_tokens=True)

outputs = pipe(messages, max_new_tokens = 1024, streamer=streamer)
```



#### 1.5 æœ¬åœ°å¾®è°ƒğŸ¤“

##### åªéœ€è¦ä¿å­˜æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œç„¶åä½¿ç”¨1.4.5çš„ä»£ç å¹¶ä¸”ä¿®æ”¹ä¸€ä¸‹æœ¬åœ°æ–‡ä»¶è·¯å¾„å°±å¯ä»¥è¿è¡Œäº†

![image-20241224183606077](./imgs/modelscope_images/image-20241224183606077.png)