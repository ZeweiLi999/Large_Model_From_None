# é­”æ­ç¤¾åŒºå¾®è°ƒ ğŸŒ…

**ç›¸å¯¹äºcolabå¾®è°ƒï¼Œå›½å†…çš„é­”æ­ç¤¾åŒºåŒæ ·ä¹Ÿå¯ä»¥è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ¬¡åœ¨é­”æ­å¾®è°ƒçš„ä»£ç æ›´ä¸ºç®€å•ã€‚**



## å¾®è°ƒå®ç°ğŸ™‚

### 1.1 æ‰“å¼€ä¸ªäººé¦–é¡µï¼Œé€‰æ‹©æ–¹å¼äºŒï¼Œå¯åŠ¨GPUç¯å¢ƒğŸ˜Š

![image-20241218104113570](./imgs/modelscope_images/image-20241218104113570.png)



### 1.2 åˆ›å»ºnotebookæ–‡ä»¶ğŸ˜‡

#### ç‚¹å‡»ä¸­å¤®Notebookçš„Python3åˆ›å»ºå³å¯

![image-20241218104503469](./imgs/modelscope_images/image-20241218104503469.png)



### 1.3 æ–‡ä»¶é‡å‘½åğŸ˜›

#### å·¦ä¾§æ–‡ä»¶å³é”®ï¼Œç‚¹å‡»Renameè¿›è¡Œé‡å‘½å

![image-20241218104942210](./imgs/modelscope_images/image-20241218104942210.png)



### 1.3 åˆ›å»ºä»£ç è¡ŒğŸ¤ª

#### ç‚¹å‡»ç¬¬äºŒè¡Œå·¥å…·æ ï¼Œç¬¬äºŒä¸ªå·¥å…·+å·è¿›è¡Œåˆ›å»º

![image-20241218104747056](./imgs/modelscope_images/image-20241218104747056.png)



### 1.4 ä»£ç æ‰§è¡ŒğŸ¤¨

**åœ¨ä»£ç æ‰§è¡Œå‰ï¼Œé¦–å…ˆå¯¼å…¥éœ€è¦ä½¿ç”¨çš„æ•°æ®é›†ï¼Œç‚¹å‡»ä¸­é—´æŒ‰é’®ä¸Šä¼ æ•°æ®é›†ï¼ˆæœ¬æ¬¡ä½¿ç”¨çš„æ•°æ®é›†ä¸ºç”„å¬›ä¼ çš„æ•°æ®é›†ï¼Œå¦‚æœä½ å–œæ¬¢å…¶ä»–çš„æ•°æ®é›†ï¼Œå¯ä»¥åœ¨ç½‘ä¸Šè¿›è¡Œæœç´¢ï¼Œç„¶åå¯¼å…¥å…¶ä¸­ï¼Œä¸è¿‡éœ€è¦ä¿è¯jsonæ–‡ä»¶æ ¼å¼ç›¸åŒï¼‰**

![image-20241218110803764](./imgs/modelscope_images/image-20241218110803764.png)

#### 1.4.1 å¯¹æ¨¡å‹è¿›è¡Œä¸‹è½½ï¼Œå¦‚æœæœ‰éœ€è¦å¯ä»¥åœ¨é­”æ­ç¤¾åŒºä¸‹è½½è‡ªå·±å–œæ¬¢çš„æ¨¡å‹ğŸ˜Œ

```
from datasets import Dataset
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig
from peft import LoraConfig, TaskType, get_peft_model
import torch

df = pd.read_json('./huanhuan.json')
ds = Dataset.from_pandas(df)

print(ds[:3])

from modelscope import snapshot_download

model_dir = snapshot_download('LLM-Research/Llama-3.2-3B-Instruct')
#../llama3-ft/models/model/LLM-Research/Meta-Llama-3-8B-Instruct
```



#### 1.4.2 æ¨¡å‹è®­ç»ƒ ğŸ˜®â€ğŸ’¨

##### åœ¨é­”æ­å¹³å°å¯ä»¥ä¿®æ”¹å°†ä»¥ä¸‹ä»£ç åˆ†ä¸ºå¤šä¸ªä»£ç å—ï¼Œåˆ†å¸ƒæ‰§è¡Œï¼Œå¯ä»¥é€šè¿‡printå†…å®¹æ¥æ£€éªŒå‡†ç¡®ä¸å¦
##### ä¸ºäº†æ–¹ä¾¿æ¼”ç¤ºï¼Œåœ¨æ­¤åªè®¾ç½®äº†3ä¸ªepochï¼Œæƒ³è¦æå‡è®­ç»ƒæ•ˆæœï¼Œå¯ä»¥å¢åŠ ä»¥ä¸‹ä»£ç è®­ç»ƒå‚æ•°ä¸­çš„è®­ç»ƒè½®æ•°
```
# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained('/mnt/workspace/.cache/modelscope/hub/LLM-Research/Llama-3.2-3B-Instruct', use_fast=False, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token  # è®¾ç½®pad_tokenä¸ºeos_token

# è¾“å‡ºeos_token_idï¼Œç¡®è®¤tokenizerå·²ç»æ­£ç¡®åŠ è½½
print(tokenizer.eos_token_id)

# 128009

# å¤„ç†æ¯ä¸ªç¤ºä¾‹çš„æ•°æ®é¢„å¤„ç†å‡½æ•°
def process_func(example):
    MAX_LENGTH = 384  # è®¾ç½®æœ€å¤§é•¿åº¦ï¼ŒLlamaåˆ†è¯å™¨å¯èƒ½å°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ªtokenï¼Œå› æ­¤éœ€è¦æ›´å¤§çš„æœ€å¤§é•¿åº¦æ¥ä¿è¯æ•°æ®å®Œæ•´æ€§
    input_ids, attention_mask, labels = [], [], []

    # ç¼–ç æŒ‡ä»¤å’Œè¾“å…¥éƒ¨åˆ†
    instruction = tokenizer(f"<|start_header_id|>user<|end_header_id|>\n\n{example['instruction'] + example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n", add_special_tokens=False)
    # ç¼–ç è¾“å‡ºéƒ¨åˆ†
    response = tokenizer(f"{example['output']}<|eot_id|>", add_special_tokens=False)

    # æ‹¼æ¥input_idsã€attention_maskå’Œlabels
    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.pad_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]  # eos tokenä¹Ÿæ˜¯è¦å…³æ³¨çš„ï¼Œå› æ­¤è®¾ä¸º1
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.pad_token_id]

    # å¦‚æœinput_idsè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ™æˆªæ–­
    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

# åº”ç”¨å¤„ç†å‡½æ•°åˆ°æ•´ä¸ªæ•°æ®é›†
tokenized_id = ds.map(process_func, remove_columns=ds.column_names)

# è¾“å‡ºç»è¿‡å¤„ç†çš„ç¬¬ä¸€ä¸ªç¤ºä¾‹ï¼Œç¡®ä¿tokenizeræ­£å¸¸å·¥ä½œ
print(tokenizer.decode(tokenized_id[0]['input_ids']))

# <|start_header_id|>user<|end_header_id|>
# å°å§ï¼Œåˆ«çš„ç§€å¥³éƒ½åœ¨æ±‚ä¸­é€‰ï¼Œå”¯æœ‰å’±ä»¬å°å§æƒ³è¢«æ’‚ç‰Œå­ï¼Œè©è¨ä¸€å®šè®°å¾—çœŸçœŸå„¿çš„â€”â€”<|eot_id|><|start_header_id|>assistant<|end_header_id|>
# å˜˜â€”â€”éƒ½è¯´è®¸æ„¿è¯´ç ´æ˜¯ä¸çµçš„ã€‚<|eot_id|><|eot_id|>

tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1]["labels"])))

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('/mnt/workspace/.cache/modelscope/hub/LLM-Research/Llama-3.2-3B-Instruct', device_map="auto", torch_dtype=torch.bfloat16)
model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½

# è¾“å‡ºæ¨¡å‹çš„æ•°æ®ç±»å‹
print(model.dtype)
# torch.bfloat16

# è®¾ç½®Loraé…ç½®
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # è®¾ç½®ä»»åŠ¡ç±»å‹ä¸ºå› æœè¯­è¨€å»ºæ¨¡
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # è®¾ç½®éœ€è¦åº”ç”¨Loraçš„æ¨¡å—
    inference_mode=False,  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    r=8,  # Loraç§©
    lora_alpha=32,  # Lora alphaï¼Œå…·ä½“ä½œç”¨å‚è§LoraåŸç†
    lora_dropout=0.1  # Loraçš„dropoutæ¯”ä¾‹
)

# å°†Loraé…ç½®åº”ç”¨åˆ°æ¨¡å‹
model = get_peft_model(model, config)

# è®¾ç½®è®­ç»ƒå‚æ•°
args = TrainingArguments(
    output_dir="./output/llama3",  # æ¨¡å‹ä¿å­˜çš„è¾“å‡ºè·¯å¾„
    per_device_train_batch_size=4,  # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒbatchå¤§å°
    gradient_accumulation_steps=4,  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    logging_steps=100,  # æ¯100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    num_train_epochs=3,  # è®­ç»ƒè½®æ•°
    save_steps=200,  # æ¯200æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    learning_rate=1e-4,  # å­¦ä¹ ç‡
    save_on_each_node=True,  # åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šä¿å­˜æ¨¡å‹
    gradient_checkpointing=True  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
)

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_id,  # è®­ç»ƒæ•°æ®é›†
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),  # æ•°æ®æ•´ç†å™¨ï¼Œè‡ªåŠ¨å¡«å……æ•°æ®
)

# å¼€å§‹è®­ç»ƒ
print("training")
trainer.train()

# ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨
peft_model_id = "./llama3_lora"
trainer.model.save_pretrained(peft_model_id)
tokenizer.save_pretrained(peft_model_id)

```



#### 1.4.3 ç‚¹å‡»è¿è¡ŒæŒ‰é’®æ‰§è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹å›¾è®­ç»ƒå®Œæˆä¸”ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡å’Œåˆ†è¯å™¨ğŸ¤¤

![image-20241218110329649](./imgs/modelscope_images/image-20241218110329649.png)



#### 1.4.4 åœ¨ä¿å­˜å¥½è®­ç»ƒå®Œçš„æƒé‡å’Œåˆ†è¯å™¨åï¼Œå°±å¯ä»¥è¿›è¡Œé—®ç­”å’¯ğŸ¤¯

```
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from peft import PeftModel

# å¦‚æœéœ€è¦åœ¨æœ¬åœ°è¿è¡Œï¼Œä¿®æ”¹ä¸€ä¸‹è·¯å¾„å°±å¯ä»¥äº†
mode_path = '/mnt/workspace/.cache/modelscope/hub/LLM-Research/Llama-3.2-3B-Instruct'
lora_path = './llama3_lora'

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(mode_path)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('/mnt/workspace/.cache/modelscope/hub/LLM-Research/Llama-3.2-3B-Instruct', device_map="auto",torch_dtype=torch.bfloat16, offload_folder="./offload")

# åŠ è½½loraæƒé‡
model = PeftModel.from_pretrained(model, model_id='./llama3_lora')

# å®šä¹‰ç”¨æˆ·çš„æé—®  
prompt = "ä½ æ˜¯è°ï¼Ÿ"  

# åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«ç³»ç»Ÿè§’è‰²å’Œç”¨æˆ·è¾“å…¥  
messages = [  
    {"role": "system", "content": "ç°åœ¨ä½ è¦æ‰®æ¼”çš‡å¸èº«è¾¹çš„å¥³äºº--ç”„å¬›"},  # ç³»ç»Ÿè§’è‰²æ˜¯ç”„å¬›  
    {"role": "user", "content": prompt}  # ç”¨æˆ·è¾“å…¥çš„æé—®  
]  

# ä½¿ç”¨tokenizerå°†æ¶ˆæ¯è½¬æ¢ä¸ºé€‚åˆæ¨¡å‹çš„è¾“å…¥æ ¼å¼  
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  

# å‡†å¤‡æ¨¡å‹è¾“å…¥ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶è½¬ç§»åˆ°GPUä¸Š  
model_inputs = tokenizer([text], return_tensors="pt").to('cuda')  

# ç”Ÿæˆå›å¤  
generated_ids = model.generate(  
    model_inputs.input_ids,  # è¾“å…¥çš„ID  
    max_new_tokens=512,  # æœ€å¤§ç”Ÿæˆtokenæ•°é‡  
    do_sample=True,  # è¿›è¡Œé‡‡æ ·ç”Ÿæˆ  
    top_p=0.9,  # æ ¸é‡‡æ ·ç­–ç•¥  
    temperature=0.5,  # ç”Ÿæˆçš„åˆ›é€ æ€§  
    repetition_penalty=1.1,  # é‡å¤æƒ©ç½šç³»æ•°  
    eos_token_id=tokenizer.encode('<|eot_id|>')[0],  # ç»“æŸtoken ID  
)  

# ä»ç”Ÿæˆçš„IDsä¸­æå–å‡ºç”Ÿæˆçš„æ–‡æœ¬éƒ¨åˆ†  
generated_ids = [  
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)  
]  

# è§£ç ç”Ÿæˆçš„IDsä¸ºå¯è¯»æ–‡æœ¬  
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]  

# æ‰“å°ç”Ÿæˆçš„å“åº”  
print(response)

```



#### å¦‚ä¸‹å°±æ˜¯ç”Ÿæˆçš„å“åº”ï¼š



![image-20241220221546128](./imgs/modelscope_images/image-20241220221546128.png)



#### 1.4.5 å¦‚æœåœ¨å¤šæ¬¡å¾®è°ƒåå¯èƒ½ä¼šå‡ºç°æ˜¾å­˜å ç”¨çš„é—®é¢˜ï¼Œè¿™ä¸ªæ—¶å€™åªéœ€è¦è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œæ¸…ç©ºä¸€ä¸‹å°±å¯ä»¥å•¦ï¼ğŸ˜‡

```
torch.cuda.empty_cache()
```



## 2. æœ¬åœ°å¾®è°ƒğŸ¤“

**å¦‚æœä½ éœ€è¦åœ¨æœ¬åœ°è¿›è¡Œè¿è¡Œï¼Œç‚¹å‡»å¦‚ä¸‹å·¦ä¾§llama3_loraæ–‡ä»¶å¤¹ï¼Œå°†å…¶ä¸­æ–‡ä»¶è¿›è¡Œä¿å­˜ï¼Œç„¶åè¿è¡Œ1.4.4ä»£ç åŒæ—¶åªéœ€è¦ä¿®æ”¹ä¸€ä¸‹å…¶ä¸­çš„æ–‡ä»¶è·¯å¾„å°±å¯ä»¥è¿›è¡Œè¿è¡Œäº†**

![image-20241220222325123](./imgs/modelscope_images/image-20241220222325123.png)

![image-20241220223711470](./imgs/modelscope_images/image-20241220223711470.png)



**å¦‚æœæƒ³è¦é—®å…¶ä»–é—®é¢˜ï¼Œä¿®æ”¹ä¸€ä¸‹æé—®è¯å³å¯ï¼Œå¦‚ä¸‹å°±ä¸ºæœ¬åœ°è¿è¡Œï¼Œé‡åˆ°å¦‚ä¸‹é—®é¢˜å¯ä»¥å¿½ç•¥ï¼š**

![image-20241220223457171](./imgs/modelscope_images/image-20241220223457171.png)